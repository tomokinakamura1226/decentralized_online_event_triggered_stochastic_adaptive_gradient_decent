{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomokinakamura\\anaconda3\\envs\\robots_town\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 指定されたプロシージャが見つかりません。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # PILイメージをPyTorchテンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,))  # データを正規化\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "N = 10\n",
    "agents_models = {}\n",
    "x = {}\n",
    "for i in range(N):\n",
    "    # 多クラスロジスティック回帰モデルを定義\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),  # 入力画像をフラット化\n",
    "        nn.Linear(28*28, 10)  # MNISTの画像サイズは28x28、出力クラスは10\n",
    "    )\n",
    "    agents_models[i] = model\n",
    "    params = [param.view(-1) for param in model.parameters()]\n",
    "    flat_params = torch.cat(params)\n",
    "    x[i] = flat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "train_dataset_size = len(train_dataset)\n",
    "test_dataset_size = len(test_dataset)\n",
    "train_per_agent_size = train_dataset_size // N\n",
    "test_per_agent_size = test_dataset_size // N\n",
    "train_sizes = [train_per_agent_size] * N\n",
    "test_sizes = [test_per_agent_size] * N\n",
    "# 最後のエージェントが余剰分を取得するように調整\n",
    "train_sizes[-1] += train_dataset_size - sum(train_sizes)\n",
    "test_sizes[-1] += test_dataset_size - sum(test_sizes)\n",
    "# データセットをランダムに分割\n",
    "train_agents_datasets = random_split(train_dataset, train_sizes)\n",
    "test_agents_datasets = random_split(test_dataset, test_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shapes = [param.shape for param in agents_models[0].parameters()]\n",
    "param_elements = [param.numel() for param in agents_models[0].parameters()]\n",
    "\n",
    "def unflatten_parameters(flat_params, param_shapes, param_elements):\n",
    "    params = []\n",
    "    offset = 0\n",
    "    for shape, elements in zip(param_shapes, param_elements):\n",
    "        param = flat_params[offset:offset + elements].view(shape)\n",
    "        params.append(param)\n",
    "        offset += elements\n",
    "    return params\n",
    "\n",
    "def assign_params_to_model(model, params):\n",
    "    with torch.no_grad():  # 勾配計算を無効化\n",
    "        for param, new_param in zip(model.parameters(), params):\n",
    "            param.data = new_param\n",
    "\n",
    "def evaluate_model(model, test_dataset, batch_size=8):\n",
    "    # テストデータセットからデータローダーを作成\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(num_epochs, E):\n",
    "\n",
    "    p = sum(i.numel() for i in agents_models[0].parameters())\n",
    "    m = {i: torch.zeros(p) for i in range(N)}\n",
    "    v = {i: torch.zeros(p) for i in range(N)}\n",
    "    v_hat = {i: torch.zeros(p) for i in range(N)}\n",
    "    y = {i: torch.zeros(p) for i in range(N)}\n",
    "    x_half = {i: torch.zeros(p) for i in range(N)}\n",
    "    y_half = {i: torch.zeros(p) for i in range(N)}\n",
    "    x_tilde = {i: torch.zeros(p) for i in range(N)}\n",
    "    y_tilde = {i: torch.zeros(p) for i in range(N)}\n",
    "    v_hat_before =  {i: torch.zeros(p) for i in range(N)}\n",
    "    u = {i: torch.zeros(p) for i in range(N)}\n",
    "    beta1, beta2, beta3 = 0.9, 0.99, 0.9\n",
    "    lamb = 0.97\n",
    "    alpha = 0.001\n",
    "    epsilon = 1e-8\n",
    "    batch_size = 16\n",
    "\n",
    "    trigger = [ 0 for i in range(N) ]\n",
    "    W = [[1/N for i in range(N)] for j in range(N)]\n",
    "    agents_loss_history = {i: [] for i in range(N)}\n",
    "    trigger_history = [[0 for i in range(num_epochs)] for j in range(N)]\n",
    "    accuracy_list = [[0 for i in range(num_epochs)] for j in range(N)]\n",
    "    for epoch in range(num_epochs):\n",
    "        agents_gradients = {i: [] for i in range(N)}  # エージェントごとの勾配を保存する辞書\n",
    "        agents_losses = {i: [] for i in range(N)} \n",
    "        learning_rate = alpha / (epoch + 1 )\n",
    "        epoch_losses = {i: 0.0 for i in range(N)}\n",
    "        epoch_counts = {i: 0 for i in range(N)}\n",
    "        print(epoch)\n",
    "        for i in range(N):\n",
    "            model = agents_models[i]\n",
    "            train_dataset = train_agents_datasets[i]\n",
    "            \n",
    "            # 指定された範囲からランダムにインデックスを選択\n",
    "            start_idx = (batch_size* 2 * epoch) % len(train_dataset)\n",
    "            end_idx = (batch_size* 2 * (epoch + 1)) % len(train_dataset)\n",
    "            if start_idx >= end_idx:\n",
    "                mnp = start_idx\n",
    "                start_idx = end_idx\n",
    "                end_idx = mnp\n",
    "            selected_indices = np.random.choice(range(start_idx, end_idx), size= 2*batch_size, replace=False)\n",
    "            \n",
    "            # 選択されたインデックスに基づいてデータセットのサブセットを作成\n",
    "            subset = Subset(train_dataset, selected_indices)\n",
    "            \n",
    "            # サブセットからデータローダーを作成（バッチサイズは1に設定して、個々のサンプルを取得）\n",
    "            dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "            batch_losses = []  # バッチごとの損失を格納するリスト\n",
    "            \n",
    "            # データローダーからバッチを作成\n",
    "            inputs_batch, labels_batch = [], []\n",
    "            \n",
    "            for inputs, labels in dataloader:\n",
    "                model.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = loss_function(output, labels)\n",
    "                loss.backward()\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "                gradients = torch.cat([param.grad.view(-1) for param in model.parameters() if param.grad is not None], 0)\n",
    "                agents_gradients[i].append(gradients)\n",
    "                epoch_losses[i] += loss.item()\n",
    "                epoch_counts[i] += 1\n",
    "\n",
    "            # エージェントの勾配リストを平均化\n",
    "            agents_gradients[i] = torch.mean(torch.stack(agents_gradients[i]), dim=0)\n",
    "            agents_losses[i] = np.mean(batch_losses)\n",
    "        \n",
    "        for i in range(N):\n",
    "            if epoch_counts[i] > 0:\n",
    "                average_loss = epoch_losses[i] / epoch_counts[i]\n",
    "                agents_loss_history[i].append(average_loss)\n",
    "        \n",
    "        for i in range(N):\n",
    "            m[i] = beta1* lamb** (epoch) * m[i] + (1- beta1* lamb**(epoch))* agents_gradients[i]\n",
    "            #print(m[i])\n",
    "            v[i] = beta2 * v[i] + (1- beta2)* agents_gradients[i] * agents_gradients[i]\n",
    "            #print(v[i])\n",
    "            v_hat_before[i] = v_hat[i]\n",
    "            v_hat[i] = beta3 * v_hat[i] + (1 - beta3) * torch.max(v_hat_before[i], v[i])\n",
    "            x_half[i] = x[i]\n",
    "            y_half[i] = y_tilde[i]\n",
    "            for j in range(N):\n",
    "                if i != j:  # 自分自身は除外\n",
    "                    x_half[i] += W[i][j] * (x_tilde[j] - x_tilde[i])\n",
    "                    y_half[i] += W[i][j] * (y_tilde[j] - y_tilde[i])\n",
    "            \n",
    "            u[i] = y_half[i] + epsilon\n",
    "            u[i] = 1 / (torch.sqrt(u[i]))\n",
    "            x[i] = x_half[i] - alpha * u[i] * m[i]\n",
    "            y[i] = y_half[i] + v_hat[i] - v_hat_before[i]\n",
    "            new_param = unflatten_parameters(x[i], param_shapes, param_elements)\n",
    "            assign_params_to_model(agents_models[i], new_param)\n",
    "\n",
    "            for i in range(N):\n",
    "                if (torch.norm(x[i] - x_tilde[i]) > E*(1/((epoch+1)*(1)))) or (torch.norm(y[i] - y_tilde[i]) > E*(1/((epoch+1)*(1)))):\n",
    "                    trigger[i] += 1\n",
    "                    x_tilde[i] = x[i]\n",
    "                    y_tilde[i] = y[i]\n",
    "                else:\n",
    "                    x_tilde[i] = x_tilde[i]\n",
    "                    y_tilde[i] = y_tilde[i]\n",
    "                trigger_history[i][epoch] = trigger[i]\n",
    "                \n",
    "            for i in range(N):\n",
    "                model = agents_models[i]\n",
    "                test_dataset = test_agents_datasets[i]\n",
    "                accuracy = evaluate_model(model, test_dataset)\n",
    "                accuracy_list[i][epoch] = accuracy\n",
    "            \n",
    "    return  trigger_history,  accuracy_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots_town",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
