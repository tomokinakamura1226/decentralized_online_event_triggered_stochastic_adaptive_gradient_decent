{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomokinakamura\\anaconda3\\envs\\robots_town\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 指定されたプロシージャが見つかりません。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "# CIFAR-10データセットの基本変換\n",
    "transform_base = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Tensorに変換\n",
    "])\n",
    "train_cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_base)\n",
    "test_cifar10_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomokinakamura\\anaconda3\\envs\\robots_town\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tomokinakamura\\anaconda3\\envs\\robots_town\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch.nn import init\n",
    "N = 5\n",
    "agents_models = {}\n",
    "for i in range(N):\n",
    "    # ResNet18モデルを定義してエージェントに割り当て\n",
    "    model = models.resnet18(pretrained=False)  # pretrainedをFalseに設定\n",
    "    model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 10)  # 出力層を10クラスに変更\n",
    "    for param in model.parameters():\n",
    "        init.constant_(param, 0)\n",
    "\n",
    "    agents_models[i] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "train_dataset_size = len(train_cifar10_dataset)\n",
    "per_agent_size = train_dataset_size // N\n",
    "sizes = [per_agent_size] * N\n",
    "sizes[-1] += train_dataset_size - sum(sizes)\n",
    "train_agents_datasets = random_split(train_cifar10_dataset, sizes)\n",
    "test_dataset_size = len(test_cifar10_dataset)\n",
    "test_per_agent_size = test_dataset_size // N\n",
    "test_sizes = [test_per_agent_size] * N\n",
    "test_sizes[-1] += test_dataset_size - sum(test_sizes)\n",
    "test_agents_datasets = random_split(test_cifar10_dataset, test_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shapes = [param.shape for param in agents_models[0].parameters()]\n",
    "param_elements = [param.numel() for param in agents_models[0].parameters()]\n",
    "\n",
    "def unflatten_parameters(flat_params, param_shapes, param_elements):\n",
    "    params = []\n",
    "    offset = 0\n",
    "    for shape, elements in zip(param_shapes, param_elements):\n",
    "        param = flat_params[offset:offset + elements].view(shape)\n",
    "        params.append(param)\n",
    "        offset += elements\n",
    "    return params\n",
    "\n",
    "def assign_params_to_model(model, params):\n",
    "    with torch.no_grad():  # 勾配計算を無効化\n",
    "        for param, new_param in zip(model.parameters(), params):\n",
    "            param.data = new_param\n",
    "\n",
    "def evaluate_model(model, test_dataset, batch_size=8):\n",
    "    # テストデータセットからデータローダーを作成\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(num_epochs, E):\n",
    "\n",
    "    p = sum(i.numel() for i in agents_models[0].parameters())\n",
    "    m = {i: torch.zeros(p) for i in range(N)}\n",
    "    v = {i: torch.zeros(p) for i in range(N)}\n",
    "    v_hat = {i: torch.zeros(p) for i in range(N)}\n",
    "    x = {i: torch.zeros(p) for i in range(N)}\n",
    "    y = {i: torch.zeros(p) for i in range(N)}\n",
    "    x_half = {i: torch.zeros(p) for i in range(N)}\n",
    "    y_half = {i: torch.zeros(p) for i in range(N)}\n",
    "    #x_tilde = {i: torch.zeros(N, p) for i in range(N)}\n",
    "    #y_tilde = {i: torch.zeros(N, p) for i in range(N)}\n",
    "    x_tilde = {i: torch.zeros(p) for i in range(N)}\n",
    "    y_tilde = {i: torch.zeros(p) for i in range(N)}\n",
    "    v_hat_before =  {i: torch.zeros(p) for i in range(N)}\n",
    "    u = {i: torch.zeros(p) for i in range(N)}\n",
    "    beta1, beta2, beta3 = 0.9, 0.99, 0.7\n",
    "    lamb = 0.97\n",
    "    alpha = 0.005\n",
    "    epsilon = 1e-8\n",
    "    batch_size = 16\n",
    "\n",
    "    trigger = [ 0 for i in range(N) ]\n",
    "    W = [[1/N for i in range(N)] for j in range(N)]\n",
    "    agents_loss_history = {i: [] for i in range(N)}\n",
    "    trigger_history = [[0 for i in range(num_epochs)] for j in range(N)]\n",
    "    accuracy_list = [[0 for i in range(num_epochs)] for j in range(N)]\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        agents_gradients = {i: [] for i in range(N)}  # エージェントごとの勾配を保存する辞書\n",
    "        agents_losses = {i: [] for i in range(N)} \n",
    "        learning_rate = alpha / (epoch + 1 )\n",
    "        epoch_losses = {i: 0.0 for i in range(N)}\n",
    "        epoch_counts = {i: 0 for i in range(N)}\n",
    "        \n",
    "        for i in range(N):\n",
    "            model = agents_models[i]\n",
    "            train_dataset = train_agents_datasets[i]\n",
    "            \n",
    "            # 指定された範囲からランダムにインデックスを選択\n",
    "            start_idx = (batch_size* 2 * epoch) % len(train_dataset)\n",
    "            end_idx = (batch_size* 2 * (epoch + 1)) % len(train_dataset)\n",
    "            if start_idx >= end_idx:\n",
    "                mnp = start_idx\n",
    "                start_idx = end_idx\n",
    "                end_idx = mnp\n",
    "            selected_indices = np.random.choice(range(start_idx, end_idx), size= 2*batch_size, replace=False)\n",
    "            \n",
    "            # 選択されたインデックスに基づいてデータセットのサブセットを作成\n",
    "            subset = Subset(train_dataset, selected_indices)\n",
    "            \n",
    "            # サブセットからデータローダーを作成（バッチサイズは1に設定して、個々のサンプルを取得）\n",
    "            dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "            batch_losses = []  # バッチごとの損失を格納するリスト\n",
    "            \n",
    "            # データローダーからバッチを作成\n",
    "            inputs_batch, labels_batch = [], []\n",
    "            \n",
    "            for inputs, labels in dataloader:\n",
    "                model.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = loss_function(output, labels)\n",
    "                loss.backward()\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "                gradients = torch.cat([param.grad.view(-1) for param in model.parameters() if param.grad is not None], 0)\n",
    "                agents_gradients[i].append(gradients)\n",
    "                epoch_losses[i] += loss.item()\n",
    "                epoch_counts[i] += 1\n",
    "\n",
    "            # エージェントの勾配リストを平均化\n",
    "            agents_gradients[i] = torch.mean(torch.stack(agents_gradients[i]), dim=0)\n",
    "            agents_losses[i] = np.mean(batch_losses)\n",
    "        \n",
    "        for i in range(N):\n",
    "            if epoch_counts[i] > 0:\n",
    "                average_loss = epoch_losses[i] / epoch_counts[i]\n",
    "                agents_loss_history[i].append(average_loss)\n",
    "        \n",
    "        for i in range(N):\n",
    "            m[i] = beta1* lamb** (epoch) * m[i] + (1- beta1* lamb**(epoch))* agents_gradients[i]\n",
    "            v[i] = beta2 * v[i] + (1- beta2)* agents_gradients[i] * agents_gradients[i]\n",
    "            v_hat_before[i] = v_hat[i]\n",
    "            v_hat[i] = beta3 * v_hat[i] + (1 - beta3) * torch.max(v_hat_before[i], v[i])\n",
    "            x_half[i] = x[i]\n",
    "            y_half[i] = y_tilde[i]\n",
    "            for j in range(N):\n",
    "                if i != j:  \n",
    "                    x_half[i] += W[i][j] * (x_tilde[j] - x_tilde[i])\n",
    "                    y_half[i] += W[i][j] * (y_tilde[j] - y_tilde[i])\n",
    "            \n",
    "            u[i] = y_half[i] + epsilon\n",
    "            u[i] = 1 / (torch.sqrt(u[i]))\n",
    "            x[i] = x_half[i] - alpha * u[i] * m[i]\n",
    "            y[i] = y_half[i] + v_hat[i] - v_hat_before[i]\n",
    "\n",
    "            new_param = unflatten_parameters(x[i], param_shapes, param_elements)\n",
    "            assign_params_to_model(agents_models[i], new_param)\n",
    "\n",
    "            for i in range(N):\n",
    "                if (torch.norm(x[i] - x_tilde[i]) > E*(1/((epoch+1)*(1/2)))) or  (torch.norm(y[i] - y_tilde[i]) > E*(1/((epoch+1)*(1/2)))):\n",
    "                    trigger[i] += 1\n",
    "                    x_tilde[i] = x[i]\n",
    "                    y_tilde[i] = y[i]\n",
    "                else:\n",
    "                    x_tilde[i] = x_tilde[i]\n",
    "                    y_tilde[i] = y_tilde[i]\n",
    "                trigger_history[i][epoch] = trigger[i]\n",
    "                \n",
    "            for i in range(N):\n",
    "                model = agents_models[i]\n",
    "                test_dataset = test_agents_datasets[i]\n",
    "                accuracy = evaluate_model(model, test_dataset)\n",
    "                accuracy_list[i][epoch] = accuracy\n",
    "            \n",
    "    return  trigger_history,  accuracy_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots_town",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
